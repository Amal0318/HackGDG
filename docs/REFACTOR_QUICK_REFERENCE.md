# VitalX Architecture Refactor ‚Äî Quick Reference

**TL;DR: From Rule-Based to ML-Driven Streaming Architecture**

---

## üìä BEFORE vs AFTER COMPARISON

### Data Flow Architecture

#### ‚ùå BEFORE (Current State)

```
Vital Simulator (State Machine)
    ‚îú‚îÄ‚îÄ STABLE ‚Üí EARLY ‚Üí CRITICAL states
    ‚îú‚îÄ‚îÄ Hardcoded transitions
    ‚îî‚îÄ‚îÄ Unrealistic spikes
         ‚Üì
    Kafka "vitals"
         ‚Üì
Pathway Engine
    ‚îú‚îÄ‚îÄ Feature engineering
    ‚îú‚îÄ‚îÄ ‚ùå Risk calculation (DUPLICATE LOGIC)
    ‚îî‚îÄ‚îÄ ‚ùå Medical state assignment
         ‚Üì
    Kafka "vitals_enriched" (with risk_score)
         ‚Üì
ML Service (confused role)
    ‚îú‚îÄ‚îÄ Has model but doesn't use enriched data
    ‚îî‚îÄ‚îÄ ‚ùå Disconnected from main flow
         ‚Üì
Backend API
    ‚îú‚îÄ‚îÄ ‚ùå ALSO calculates risk (SECOND DUPLICATE)
    ‚îú‚îÄ‚îÄ Standalone model inference
    ‚îî‚îÄ‚îÄ Conflicting risk authorities
         ‚Üì
    Frontend (hardcoded states)
    
Separate:
RAG Service (ChromaDB)
    ‚îú‚îÄ‚îÄ ‚ùå Batch indexing (not real-time)
    ‚îú‚îÄ‚îÄ Kafka consumer ‚Üí batch rebuild
    ‚îî‚îÄ‚îÄ Standalone vector DB
```

**Problems:**
- 3 different risk calculation sources (Pathway, ML Service, Backend)
- State machine produces unrealistic vitals
- RAG is batch-updated, not streaming
- Topic explosion bug (1M+ messages/hour)
- Mixed concerns everywhere

---

#### ‚úÖ AFTER (Target State)

```
Vital Simulator (Drift Model)
    ‚îú‚îÄ‚îÄ Physiological baselines
    ‚îú‚îÄ‚îÄ Gradual drift (Brownian motion)
    ‚îú‚îÄ‚îÄ Probabilistic deterioration
    ‚îî‚îÄ‚îÄ NO states, NO spikes
         ‚Üì
    Kafka "vitals_raw"
    {patient_id, timestamp, heart_rate, systolic_bp, ..., shock_index}
         ‚Üì
Pathway Engine (3 Functions)
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ A) Feature Engineering              ‚îÇ
    ‚îÇ    ‚Ä¢ Sliding window (30-60 min)     ‚îÇ
    ‚îÇ    ‚Ä¢ Rolling statistics             ‚îÇ
    ‚îÇ    ‚Ä¢ Deltas, trends                 ‚îÇ
    ‚îÇ    ‚Ä¢ Anomaly flags                  ‚îÇ
    ‚îÇ    ‚Ä¢ ‚úÖ NO RISK SCORING             ‚îÇ
    ‚îÇ         ‚Üì                            ‚îÇ
    ‚îÇ    Kafka "vitals_enriched"          ‚îÇ
    ‚îÇ    {+ rolling_mean_hr, + hr_delta,  ‚îÇ
    ‚îÇ     + anomaly_flag}                 ‚îÇ
    ‚îÇ                                      ‚îÇ
    ‚îÇ B) Streaming Vector Index           ‚îÇ
    ‚îÇ    ‚Ä¢ Convert events ‚Üí text chunks   ‚îÇ
    ‚îÇ    ‚Ä¢ Embed in real-time             ‚îÇ
    ‚îÇ    ‚Ä¢ Sliding window (3 hrs)         ‚îÇ
    ‚îÇ    ‚Ä¢ Per-patient isolation          ‚îÇ
    ‚îÇ    ‚Ä¢ Auto-expire old data           ‚îÇ
    ‚îÇ                                      ‚îÇ
    ‚îÇ C) Query API (HTTP)                 ‚îÇ
    ‚îÇ    POST /query                       ‚îÇ
    ‚îÇ    {patient_id, query_text}         ‚îÇ
    ‚îÇ    ‚Üí Retrieved context              ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    Kafka "vitals_enriched"
         ‚Üì
ML Service (SOLE RISK AUTHORITY) ‚≠ê
    ‚îú‚îÄ‚îÄ Consume enriched data
    ‚îú‚îÄ‚îÄ Sequence buffer (60 timesteps)
    ‚îú‚îÄ‚îÄ predict(sequence) ‚Üí risk_score
    ‚îî‚îÄ‚îÄ ‚úÖ SINGLE SOURCE OF TRUTH
         ‚Üì
    Kafka "vitals_predictions"
    {patient_id, timestamp, risk_score}
         ‚Üì
Backend API (Orchestrator)
    ‚îú‚îÄ‚îÄ Merge vitals_enriched + vitals_predictions
    ‚îú‚îÄ‚îÄ WebSocket /ws ‚Üí unified stream
    ‚îú‚îÄ‚îÄ REST endpoints
    ‚îî‚îÄ‚îÄ Chat endpoint ‚Üí Query Pathway RAG ‚Üí LLM
         ‚Üì
    Frontend (ML-driven)
    ‚îú‚îÄ‚îÄ Live vitals
    ‚îú‚îÄ‚îÄ Risk score display
    ‚îú‚îÄ‚îÄ Risk trend chart
    ‚îî‚îÄ‚îÄ RAG chat panel
    
Alert Engine (Parallel)
    ‚îú‚îÄ‚îÄ Watch vitals_predictions
    ‚îú‚îÄ‚îÄ Trigger: risk_score > threshold
    ‚îî‚îÄ‚îÄ Generate alert with context
```

**Solutions:**
- ‚úÖ ML Service is sole risk authority
- ‚úÖ Pathway does deterministic features only
- ‚úÖ Streaming RAG inside Pathway (no batch indexing)
- ‚úÖ Clean separation of concerns
- ‚úÖ Linear topic growth (no explosion)

---

## üéØ RESPONSIBILITY MATRIX

| Component | BEFORE | AFTER |
|-----------|--------|-------|
| **Vital Simulator** | State machine with transitions | Drift model, gradual changes |
| **Risk Calculation** | Pathway + Backend (duplicated) | ML Service ONLY |
| **Feature Engineering** | Mixed with risk logic | Pathway ONLY |
| **RAG System** | Standalone ChromaDB (batch) | Pathway native (streaming) |
| **Medical States** | Hardcoded everywhere | REMOVED |
| **Alert Logic** | Rule-based independent system | Threshold on ML risk_score |

---

## üîë KEY DESIGN PRINCIPLES

### 1. Single Source of Truth
- **Risk Score:** ML Service publishes to `vitals_predictions`
- No other service calculates risk

### 2. Separation of Concerns
- **Pathway:** Feature engineering + RAG memory
- **ML Service:** Risk inference
- **Backend:** Orchestration + UI serving

### 3. Streaming-First
- No batch processing
- Event-driven updates
- Real-time indexing

### 4. Clean Data Flow
```
Raw ‚Üí Feature Engineering ‚Üí ML Inference ‚Üí Presentation
```

---

## üì¶ KAFKA TOPICS

| Topic | Producer | Consumer | Schema | Purpose |
|-------|----------|----------|--------|---------|
| `vitals_raw` | Vital Simulator | Pathway | {patient_id, timestamp, heart_rate, systolic_bp, ...} | Raw physiological data |
| `vitals_enriched` | Pathway | ML Service, Backend | {+ rolling_mean_hr, + hr_delta, + anomaly_flag} | Feature-engineered data |
| `vitals_predictions` | ML Service | Backend, Alert Engine | {patient_id, timestamp, risk_score} | ML risk scores |
| `alerts_stream` | Alert Engine | Frontend, Notification | {patient_id, alert_type, risk_score, context} | Alert events |

---

## üöÄ MIGRATION CHECKLIST

### Phase 1: Foundation
- [ ] Refactor Vital Simulator (drift model)
- [ ] Configure Kafka topics
- [ ] Remove state machine code

### Phase 2: Pathway Refactor
- [ ] Create feature_engineering.py
- [ ] Remove risk calculation from Pathway
- [ ] Implement streaming RAG index
- [ ] Expose query API

### Phase 3: ML Service
- [ ] Kafka consumer for vitals_enriched
- [ ] Sequence buffer implementation
- [ ] Placeholder predict() function
- [ ] Kafka producer for vitals_predictions

### Phase 4: Backend Integration
- [ ] Stream merger (join enriched + predictions)
- [ ] WebSocket handler
- [ ] REST endpoints
- [ ] Chat endpoint with Pathway RAG

### Phase 5: Frontend
- [ ] Remove hardcoded state labels
- [ ] Add risk score display
- [ ] Risk trend chart component
- [ ] RAG chat panel

### Phase 6: Production
- [ ] Clean logging (remove emojis)
- [ ] Health checks
- [ ] Monitoring (Prometheus/Grafana)
- [ ] Load testing

---

## üß™ VALIDATION SCENARIOS

### Scenario 1: Risk Score Authority
**Test:** Publish to vitals_enriched, check predictions
- ‚úÖ ML Service publishes to vitals_predictions
- ‚úÖ Backend serves risk_score from predictions topic
- ‚úÖ Frontend displays ML-driven risk
- ‚ùå No risk_score in vitals_enriched

### Scenario 2: RAG Query
**Test:** Ask "Why is shock index increasing?"
- ‚úÖ Backend calls Pathway query API
- ‚úÖ Pathway retrieves recent chunks
- ‚úÖ LLM generates grounded response
- ‚ùå No hallucination, no diagnosis

### Scenario 3: Realistic Vitals
**Test:** Run simulator for 10 minutes
- ‚úÖ Gradual trends (no spikes)
- ‚úÖ Drift rates realistic (<0.5 bpm/min)
- ‚úÖ Occasional deterioration triggers
- ‚ùå No state transitions

### Scenario 4: Topic Growth
**Test:** Run system for 1 hour
- ‚úÖ Linear growth (~28,800 events total)
  - 8 patients √ó 1 msg/sec √ó 3600 sec = 28,800
- ‚úÖ vitals_enriched has ~same count as vitals_raw
- ‚ùå Not 1M+ messages (explosion bug)

---

## üîß QUICK TROUBLESHOOTING

### Issue: No risk scores appearing
**Check:**
1. ML Service consuming vitals_enriched?
2. Sequence buffer full? (needs 60 events)
3. ML Service publishing to vitals_predictions?

### Issue: RAG query returns empty
**Check:**
1. Pathway index receiving events?
2. Patient ID exists in index?
3. Embeddings not expired? (3 hour window)

### Issue: Kafka topic explosion
**Check:**
1. Pathway using latest-state materialization?
2. Multiple groupby operations? (reduce to one)
3. Consumer lag increasing?

### Issue: WebSocket not updating
**Check:**
1. Backend consuming both topics?
2. Stream merger joining correctly?
3. WebSocket connection alive?

---

## üìö CRITICAL CODE LOCATIONS

### Remove Risk Calculation
**Files to modify:**
- `pathway-engine/app/risk_engine.py` ‚Üí ARCHIVE
- `backend-api/main.py` ‚Üí Remove model inference

### Remove State Machine
**Files to modify:**
- `vital-simulator/app/main.py` ‚Üí Delete PatientState enum
- Remove transition logic (lines ~200-400)

### Remove Standalone RAG
**Files to remove:**
- `rag-service/` ‚Üí DELETE entire folder
- Functionality moved to pathway-engine

### Add Streaming RAG
**New files:**
- `pathway-engine/app/streaming_rag.py`
- `pathway-engine/app/query_api.py`
- `pathway-engine/app/embeddings.py`

---

## üéì LEARNING RESOURCES

### Pathway Streaming
- [Pathway Documentation](https://pathway.com/developers/documentation)
- [Streaming RAG Tutorial](https://pathway.com/developers/showcases/rag-with-streaming-data)

### Kafka Best Practices
- [Idempotent Producers](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/)
- [Topic Design](https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/)

### Medical AI
- [Ethics in Medical AI](https://www.nature.com/articles/s41591-021-01614-0)
- [Physiological Modeling](https://physoc.onlinelibrary.wiley.com/doi/full/10.1113/expphysiol.2009.051748)

---

## üí° COMMON PITFALLS TO AVOID

### ‚ùå Don't Do This
1. **Multiple Risk Calculations**
   - Only ML Service should publish risk_score
   
2. **Batch RAG Indexing**
   - Use streaming updates, not periodic rebuilds
   
3. **Cross-Patient Data Leakage**
   - Ensure patient isolation in RAG index
   
4. **Hardcoded State Labels**
   - Remove STABLE/CRITICAL from UI
   
5. **Duplicate Kafka Topics**
   - Stick to 4 topics: vitals_raw, vitals_enriched, vitals_predictions, alerts_stream

### ‚úÖ Do This Instead
1. **Clear Ownership**
   - Each metric has one authoritative source
   
2. **Event-Driven Updates**
   - React to Kafka events, don't poll
   
3. **Patient Isolation**
   - Use patient_id as partition key
   
4. **Dynamic UI**
   - Display ML-driven risk scores
   
5. **Topic Hygiene**
   - Monitor growth, configure retention

---

## üö¶ GO-LIVE CHECKLIST

### Pre-Production
- [ ] All services have health checks
- [ ] Logging is production-ready (no debug noise)
- [ ] Kafka topics configured with proper retention
- [ ] Consumer groups properly named
- [ ] Docker containers restart on failure

### Production Monitoring
- [ ] Prometheus metrics exposed
- [ ] Grafana dashboards created
- [ ] Alert rules configured
- [ ] Consumer lag monitoring
- [ ] Error rate tracking

### Documentation
- [ ] API documentation (Swagger/OpenAPI)
- [ ] Deployment runbook
- [ ] Troubleshooting guide
- [ ] Architecture diagram updated

---

## üìû SUPPORT CONTACTS

For implementation questions:
- **Architecture:** See [STREAMING_ARCHITECTURE_REFACTOR.md](STREAMING_ARCHITECTURE_REFACTOR.md)
- **Code Templates:** See [IMPLEMENTATION_TEMPLATES.md](IMPLEMENTATION_TEMPLATES.md)
- **Pathway Issues:** [Pathway Discord](https://discord.gg/pathway)
- **Kafka Issues:** [Confluent Community](https://forum.confluent.io/)

---

**Version:** 1.0  
**Last Updated:** February 23, 2026  
**Status:** Ready for Implementation

---

## üéØ IMMEDIATE NEXT STEPS

1. **Start with Vital Simulator** (Phase 1)
   - Self-contained
   - No dependencies
   - Immediate visible impact

2. **Then Pathway Features** (Phase 3)
   - Remove risk calculation
   - Add feature engineering
   - Test with simulator

3. **Then ML Service** (Phase 6)
   - Add Kafka consumer
   - Implement placeholder predict()
   - Publish to vitals_predictions

4. **Integration** (Phases 7-9)
   - Backend stream merger
   - Frontend updates
   - Alert engine

5. **Polish** (Phase 10)
   - Clean logging
   - Health checks
   - Production hardening

**Estimated Timeline:**
- Phase 1: 2-3 days
- Phase 2-3: 3-4 days
- Phase 4-6: 4-5 days
- Phase 7-9: 3-4 days
- Phase 10: 2-3 days

**Total: ~2-3 weeks for complete refactor**

Good luck! üöÄ
