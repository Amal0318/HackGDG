# VitalX ML Service

LSTM-based time-series deterioration prediction model for ICU Digital Twin system.

## ğŸ“‹ Overview

This service trains and deploys an LSTM model with attention mechanism to predict patient deterioration 3-5 minutes in advance using real-time vital sign telemetry.

**Model Type**: Binary Classification (Time-Series)  
**Input**: 60-second sliding window of 14 vital features  
**Output**: Risk probability [0, 1]  
**Architecture**: LSTM + Attention + Logistic Regression Fallback

---

## ğŸ—ï¸ Project Structure

```
ml-service/
â”œâ”€â”€ data/                          # Training data (generated)
â”‚   â”œâ”€â”€ X.npy                      # Sequences (samples, 60, 14)
â”‚   â””â”€â”€ y.npy                      # Labels (samples,)
â”‚
â”œâ”€â”€ models/                        # Model architectures
â”‚   â”œâ”€â”€ lstm_model.py             # LSTM with Attention
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ training/                      # Training pipeline
â”‚   â”œâ”€â”€ generate_dataset.py       # Dataset generation from JSONL
â”‚   â”œâ”€â”€ dataset.py                # PyTorch Dataset classes
â”‚   â”œâ”€â”€ train.py                  # Main training script
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ utils/                         # Utilities
â”‚   â”œâ”€â”€ metrics.py                # Evaluation metrics
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ saved_models/                  # Trained models & artifacts
â”‚   â”œâ”€â”€ best_model.pth            # Best LSTM checkpoint
â”‚   â”œâ”€â”€ fallback_logistic.pkl     # Fallback model
â”‚   â”œâ”€â”€ scaler.pkl                # Feature scaler
â”‚   â”œâ”€â”€ feature_config.json       # Feature metadata
â”‚   â”œâ”€â”€ test_metrics.json         # Test performance
â”‚   â”œâ”€â”€ medical_metrics.json      # Clinical metrics
â”‚   â””â”€â”€ plots/                    # Evaluation plots
â”‚
â”œâ”€â”€ app/                           # FastAPI inference service
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ Dockerfile                     # Container definition
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Generate Dataset

**Prepare raw telemetry:**
- Place `vitals.jsonl` in `../data/` directory
- Format: JSONL with fields: `patient_id`, `timestamp`, vital signs

**Generate training dataset:**
```bash
cd training
python generate_dataset.py
```

**Output:**
- `data/X.npy` - Sequences (samples, 60, 14)
- `data/y.npy` - Binary labels
- `saved_models/scaler.pkl` - Feature scaler
- `saved_models/feature_config.json` - Metadata

### 3ï¸âƒ£ Train Model

```bash
cd training
python train.py
```

**Training includes:**
- LSTM with Attention training
- Logistic Regression fallback training
- Early stopping
- Learning rate scheduling
- Comprehensive evaluation

**Output:**
- `saved_models/best_model.pth` - Best LSTM model
- `saved_models/fallback_logistic.pkl` - Fallback model
- `saved_models/plots/` - Training curves & evaluation plots
- `saved_models/test_metrics.json` - Performance metrics

---

## ğŸ“Š Model Architecture

### LSTM with Attention

```
Input: (batch, 60, 14)
    â†“
LSTM (2 layers, hidden=128, dropout=0.3)
    â†“
Attention Mechanism (learns important timesteps)
    â†“
FC1 (128 â†’ 64) + Dropout(0.3)
    â†“
FC2 (64 â†’ 32) + Dropout(0.2)
    â†“
Output (32 â†’ 1) + Sigmoid
    â†“
Risk Probability [0, 1]
```

### Features (14 per timestep)

**Raw Vitals:**
1. Heart Rate
2. Systolic BP
3. Diastolic BP
4. SpO2
5. Respiratory Rate
6. Temperature
7. Shock Index

**Engineered Features:**
8. HR Delta (rate of change)
9. SBP Delta
10. SpO2 Delta
11. Shock Index Delta
12. HR Rolling Mean (10-sec window)
13. SBP Rolling Mean
14. SpO2 Rolling Mean

---

## ğŸ“ˆ Training Configuration

| Parameter | Value |
|-----------|-------|
| Batch Size | 32 |
| Epochs | 50 (with early stopping) |
| Learning Rate | 0.001 |
| Optimizer | Adam |
| Loss Function | BCELoss |
| Gradient Clipping | 1.0 |
| Early Stopping Patience | 10 epochs |
| Train/Val/Test Split | 70% / 15% / 15% |
| Balanced Sampling | Enabled |

---

## ğŸ”¬ Evaluation Metrics

### Standard Metrics
- **Accuracy**: Overall correctness
- **Precision**: Reliability of positive predictions
- **Recall**: Ability to catch deteriorations
- **F1 Score**: Harmonic mean
- **ROC-AUC**: Area under ROC curve
- **PR-AUC**: Area under Precision-Recall curve

### Medical Metrics
- **Sensitivity**: Same as Recall (key metric)
- **Specificity**: True negative rate
- **PPV**: Positive Predictive Value
- **NPV**: Negative Predictive Value
- **FNR**: False Negative Rate (critical!)
- **FPR**: False Positive Rate (alert fatigue)

### Priority
âœ… **High Recall** (minimize false negatives)  
âš ï¸ Acceptable precision (balance alert fatigue)

---

## ğŸ“¦ Saved Models

### LSTM Model (`best_model.pth`)

```python
import torch
from models.lstm_model import LSTMAttentionModel

# Load model
model = LSTMAttentionModel(input_size=14, hidden_size=128, num_layers=2)
checkpoint = torch.load('saved_models/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Inference
with torch.no_grad():
    risk, attention = model(sequence)  # sequence: (batch, 60, 14)
```

### Logistic Regression Fallback (`fallback_logistic.pkl`)

```python
import pickle

# Load model
with open('saved_models/fallback_logistic.pkl', 'rb') as f:
    data = pickle.load(f)
    lr_model = data['model']
    scaler = data['scaler']

# Inference
X_flat = X.reshape(X.shape[0], -1)
X_scaled = scaler.transform(X_flat)
risk = lr_model.predict_proba(X_scaled)[:, 1]
```

---

## ğŸ§ª Testing Models

### Test LSTM Model
```bash
cd models
python lstm_model.py
```

### Test Dataset
```bash
cd training
python dataset.py
```

### Test Metrics
```bash
cd utils
python metrics.py
```

---

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t vitalx-ml-service .

# Run container
docker run -p 8000:8000 vitalx-ml-service
```

---

## ğŸ“ Usage Examples

### Generate Dataset
```python
from training.generate_dataset import load_jsonl, build_dataset, normalize_sequences

# Load raw data
df = load_jsonl('../data/vitals.jsonl')

# Build dataset
X, y = build_dataset(df)

# Normalize
X_scaled, scaler = normalize_sequences(X)
```

### Train Model
```python
from training.train import main

# Run full training pipeline
main()
```

### Evaluate Model
```python
from utils.metrics import calculate_metrics, print_metrics_report

y_true = [0, 1, 1, 0, 1]
y_pred = [0, 1, 0, 0, 1]
y_proba = [0.2, 0.8, 0.4, 0.1, 0.9]

metrics = calculate_metrics(y_true, y_pred, y_proba)
print_metrics_report(metrics)
```

---

## ğŸ”§ Configuration

Edit `training/train.py` â†’ `Config` class:

```python
class Config:
    # Model
    hidden_size = 128
    num_layers = 2
    dropout = 0.3
    
    # Training
    batch_size = 32
    epochs = 50
    learning_rate = 0.001
    
    # Early stopping
    patience = 10
```

---

## ğŸ“Š Expected Performance

| Metric | Target | Notes |
|--------|--------|-------|
| ROC-AUC | > 0.85 | Primary metric |
| Recall | > 0.90 | Critical: catch deteriorations |
| Precision | > 0.70 | Balance alert fatigue |
| F1 Score | > 0.75 | Overall balance |
| FNR | < 0.10 | Max 10% missed cases |

---

## ğŸ” Troubleshooting

### Issue: Data files not found
**Solution**: Run `python training/generate_dataset.py` first

### Issue: Out of memory during training
**Solution**: Reduce `batch_size` in `Config`

### Issue: Model overfitting
**Solution**: 
- Increase `dropout`
- Enable data augmentation: `use_augmentation = True`
- Reduce `hidden_size`

### Issue: Poor recall
**Solution**:
- Enable balanced sampling: `use_balanced_sampling = True`
- Adjust classification threshold (default: 0.5)
- Check class distribution in dataset

---

## ğŸ“š Dependencies

See `requirements.txt` for full list:
- PyTorch >= 2.0.0
- NumPy >= 1.24.0
- Pandas >= 2.0.0
- Scikit-learn >= 1.3.0
- Matplotlib >= 3.7.0
- FastAPI == 0.104.1
- Uvicorn == 0.24.0

---

## ğŸ¤ Integration with VitalX

This ML service integrates with:
- **Vital Simulator**: Generates telemetry data
- **Pathway Engine**: Real-time stream processing
- **Backend API**: Serves predictions to frontend
- **Digital Twin**: Provides risk scores for dashboard

---

## ğŸ“„ License

Part of VitalX Real-Time ICU Digital Twin System

---

## ğŸ‘¨â€ğŸ’» Development

### Add New Features
1. Update `FEATURES` list in `generate_dataset.py`
2. Adjust `input_size` in model config
3. Regenerate dataset
4. Retrain model

### Modify Architecture
1. Edit `models/lstm_model.py`
2. Update `Config` in `training/train.py`
3. Retrain from scratch

---

## âœ… Checklist

- [x] Dataset generation script
- [x] LSTM model with attention
- [x] Logistic regression fallback
- [x] PyTorch dataset classes
- [x] Training pipeline with early stopping
- [x] Comprehensive evaluation metrics
- [x] Medical-focused reporting
- [x] Model checkpointing
- [x] Visualization plots
- [x] Docker support
- [ ] FastAPI inference endpoint (TODO)
- [ ] Real-time streaming integration (TODO)

---

**Built for VitalX Digital Twin**  
Real-Time ICU Patient Deterioration Prediction ğŸ¥
